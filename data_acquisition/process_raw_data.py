"""
Given a CSV generated by the Web Scraper IO tool, process the data for our use.
"""
import argparse
import csv
import datetime
import json
import re
import sys
from typing import Any, Callable, Dict, List, Optional, Text

from loggers import LoggerFactory

# This field_size_limit is needed to import the data CSV
max_int = sys.maxsize
while True:
  try:
    csv.field_size_limit(max_int)
    break
  except OverflowError:
    max_int = int(max_int/10)


class EntryParser():
  def __init__(self,
    output_field: Text,
    input_field: Text,
    parser: Callable[[Any], Text]
  ) -> None:
    self.output_field: Text = output_field
    self.input_field: Text = input_field
    self.parser: Callable[[Text], Text] = parser
  
  def parse(self, x: Text) -> Text:
    if (not x) or (x == "null"):
      raise ValueError("Input string is empty.")
    
    output: Text = self.parser(x)
    if output:
      return output
    else:
      raise ValueError("Parsed string is empty.")


# Start: parser function implementations
# TODO: Maybe move this into a different file. Kept this here for development.
def parser_compress_all_whitespace(x: Text) -> Text:
  return re.sub(r"\s+", " ", x)


def parser_compress_whitespace_from_json(json_key: Text) -> Callable:

  def inner_function(json_str: Text) -> Text:
    json_array = json.loads(json_str)
    if not json_array:
      raise ValueError("JSON string is not readable.")

    text_array = [
      parser_compress_all_whitespace(item[json_key])
      for item in json_array
    ]
    return "\n".join(
      item for item in text_array
      if item
    )

  return inner_function


def parser_get_year_from_string(max_year: int) -> Callable:
  def inner_function(x: Text) -> Text:
    match: Optional[re.Match] = re.search("\d{4}", x)
    if not match:
      raise ValueError(
        "Could not find a year in the following string: {}".format(x)
      )
    else:
      year_str: Text = match.group()
      year_int: int = int(year_str)
      if year_int <= max_year:
        return year_str
      
      raise ValueError(
        "Extracted year \"{}\" is greater than max_year \"{}\"".format(
          year_int, max_year
        )
      )
  
  return inner_function

# End: parser function implementations

def process_data(
  input_file_loc: Text,
  output_file_loc: Text,
) -> None:
  print("Input csv location: {}".format(input_file_loc))
  print("Output csv location: {}".format(output_file_loc))

  """
  Original csv headers:
    web-scraper-order,
    web-scraper-start-url,
    speech_list_page,
    speech_list_page-href,
    speech_page,
    speech_page-href,
    speech-speech_name,
    speech-subtitle,
    speech-speaker_name,
    speech-raw_html,
    speech-transcript_json
  """
  
  # TODO: try to parse date from the subtitle (maybe just the year)
  output_csv_parsers: List[EntryParser] = [
    EntryParser(
      output_field="title",
      input_field="speech-speech_name",
      parser=parser_compress_all_whitespace
    ),
    EntryParser(
      output_field="speaker",
      input_field="speech-speaker_name",
      parser=parser_compress_all_whitespace
    ),
    EntryParser(
      output_field="transcript",
      input_field="speech-transcript_json",
      parser=parser_compress_whitespace_from_json("speech-transcript_json")
    ),

    # Extra metadata (experimental)
    # EntryParser(
    #   output_field="subtitle",
    #   input_field="speech-subtitle",
    #   parser=parser_compress_all_whitespace
    # ),
    EntryParser(
      output_field="year",
      input_field="speech-subtitle",
      parser=parser_get_year_from_string(max_year=datetime.datetime.now().year)
    )
  ]

  output_csv_fields: List[Text] = [p.output_field for p in output_csv_parsers]

  # output_csv_fields = [
  #   "title",
  #   "speaker",
  #   "transcript",
  #   # Extra metadata (experimental)
  #   "subtitle",
  #   "year"
  # ]

  with open(input_file_loc, "r", newline="", encoding="utf-8-sig") as input_file:
    input_reader = csv.DictReader(
      input_file, quoting=csv.QUOTE_ALL
    )

    with open(output_file_loc, "w", newline="", encoding="utf-8") as output_file:
      output_writer = csv.DictWriter(
        output_file, quoting=csv.QUOTE_ALL, fieldnames=output_csv_fields
      )
      output_writer.writeheader()

      num_input_entries: int = 0
      num_output_entries: int = 0

      for entry in input_reader:
        num_input_entries += 1

        # Processing each piece of data, as needed
        try:
          output_dict: Dict[Text, Text] = {
            entry_parser.output_field : entry_parser.parse(
              entry[entry_parser.input_field]
            )
            for entry_parser in output_csv_parsers
          }

          try:
            output_writer.writerow(output_dict)
          except Exception as e:
            raise ValueError("Error writing following dict: {}\n{}".format(
              output_dict, e
            ))

          num_output_entries += 1

        except Exception as e:
          print("Error encountered, skipping:\n\tSpeech: {}\n\tSpeaker: {}\n\tLink: {}\n\tError: {}\n".format(
            parser_compress_all_whitespace(entry["speech-speech_name"]),
            parser_compress_all_whitespace(entry["speech-speaker_name"]),
            entry["speech_page-href"],
            e
          ))
      
      print("\nNumber of input entries: {}".format(num_input_entries))
      print("Number of output entries: {}".format(num_output_entries))


def parse_args() -> argparse.Namespace:
  parser: argparse.ArgumentParser = argparse.ArgumentParser(description="Process web scraped data.")
  parser.add_argument(
    "-i", "--input", type=str, required=True,
    help="Input csv location"
  )

  parser.add_argument(
    "-o", "--output", type=str, required=True,
    help="Location (csv) where to output processed data"
  )

  parser.add_argument(
    "-l", "--log", type=str, required=False,
    help="(Optional) Location where to output .txt log file"
  )

  return parser.parse_args()


def main() -> None:
  args: argparse.Namespace = parse_args()
  
  log_file_loc = args.log if args.log else ""
  with LoggerFactory(log_file_loc) as logger_factory:
    logger_factory.set_loggers()

    process_data(
      input_file_loc = args.input,
      output_file_loc = args.output
    )


if __name__ == "__main__":
  main()
